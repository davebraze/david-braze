+++
title = "Norm-referenced Test Scores"
date = 2022-01-08T00:00:00
lastmode = ""
draft = true
authors = ["David Braze"]
tags = ["data", "evaluation", "test scores"]
categories = ["Evaluation"]

[image]
  caption = ""
  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "smart"
+++

A norm-referenced test is one whose score furnishes an estimate of where an individual stands on the measured trait, relative to some pre-established sample of same-aged individuals, called the "normative sample" or "norm group". The normative sample is nothing more than a group of children who have taken the same test. Their scores are used to determine what is typical for their age group. For most published norm-referenced tests, the norm group is intended to be a nationally representative sample. But, for each age range (say, the kids in a single grade) the sample is often relatively small. For a typical published test, itt might be anywhere from a few dozen to a few hundred kids. 

Norm-referenced scores are purely descriptive. Norms are nothing more than the distribution of performance seen in the normative sample of children, which then serves as a standard to give meaning to individual student scores. Koretz (2008), especially chapter 8, includes an overview of measurement in education, and strengths and weaknesses of different approaches. It's a nice discussion pitched at a level that will be approachable to teachers and other professional educators.

All norm-referenced tests start with a student's **raw score**, which is the number of items correct or some other function of their item scores. Raw scores are transformed into norm-referenced test scores in order to put them into context, to give meaning to the scores. For tests that have more than one form and more than one level, norm-referenced scores make possible comparisons across different forms and levels of a test, and across different groups of students who take the same test. Performance of the normative sample always serves as the standard for comparison; it is the ruler against used to measure all other students.

Various transformations used for norm-referenced scores can be thought of as different *scales* for the measurement of specific traits or abilities. By analogy, you might think of the differences between meters and yards. We have different scales because they serve different purposes. Some are simpler to explain to lay people; some are better suited for comparing a student to same-aged peers, and some are better suited to tracking change over time (i.e. *progress* or *development*). Not all are well suited to the kind of numerical manipulations necessary for creating summary statistics, like mean (average) scores for a group, or for statistical modeling (as in a regression model). 

**Percentile rank scores** (PR) signify the percentage of individuals in the normative sample whose scores fell at or below a given score. Percentile ranks divide scores into 100 equal-sized groups, with higher ranks having better performance. Percentile ranks are most commonly *age-graded* which is to say that they provide a ranking relative to other individuals of similar age. So, a percentile rank of 80 is a score as high or higher than 80% of similarly aged individuals in the normative sample. **Deciles** are similar to percentile ranks, but in this case the division is into 10 groups rather than 100. The first (lowest) decile includes percentile ranks 0-9; the second decile is percentile ranks 10-19, and so on. **Stanines** are another quantile-based score. For stanines, the division is into equal-sized groups of 9. You can think of deciles and stanines as simply percentile ranks lumped into larger bins. Percentile ranks and stanines are both commonly reported score types for norm-referenced tests. Deciles are less common. They are also relatively easy to explain, but they lack the property of having equal-intervals across their range. This limits their usefulness for statistical purposes. 

Figure \@ref(fig:scales1) shows the relationship between the percentiles, deciles, and stanines to each other and to the normal distribution. It also highlights the fact that these are not equal-interval scales. Essentially, a 10 point difference at either end of the percentile scale is larger than a 10 point difference near the middle of the scale. Standard Scores, on the other hand, *ARE* an equal-interval scale.

**Standard scores** (STD) represent student student performance along a normal distribution (AKA guassian distribution, AKA 'bell curve') with specific mean and standard deviation (sd). Most commonly, the average (for the normative sample) is set to 100, and the standard deviation to 15. This makes it possible to determine how far a particular student's score is above or below the normative mean (100) in standard deviation units. A standard score of 105 is one-third of a standard deviation (five fifteenths of an sd) above the mean; a standard score of 90 is two-thirds of a standard deviation (ten fifteenths of an sd) below the mean. Like percentile ranks, standard scores are typically *age-graded*. Unlike the percentile rank, the standard score is an equal-interval scale a 10 point difference is the same, no matter where on the scale you look.

Other types of standardized scores, on different scales, do exist. For example, the **T-score** is scaled to have a mean of 50 and standard deviation of 10, while the **Normal curve equivalent** (NCE) score has a mean of 50 and a standard deviation of 21.06. **Scaled scores** used by the Gray Oral Reading Test (GORT), and others, is yet another member of the family. These have a mean of 10 and standard deviation of 3. All of these retain the equal-interval quality of standard scores and all are typically age-graded. You'll sometimes see other choices of mean and standard deviation  for standardizing scores in use among educational tests. But, all are more-or-less functionally equivalent. *The* standard score, with a mean of 100 and standard deviation of 15, is probably the most common. Standard scores may be a bit more difficult to explain or to understand than percentile rank scores but, on the other hand, they have better numerical properties in that they *do* have equal intervals across their range. However, like other age-graded scores, standard scores are better suited to comparison of a student to same-aged peers than to tracking a student's development over time. 

So, the different kinds of scales mentioned above are not ideal for tracking progress or development. Why is that? I'll use an analogy to percentile rank scores to illustrate. Imagine we have a group of 100 students all lined up in single file. Each student's "score" is their position in line (1, 2, 3, ..., 100). Now, let's march them across the school yard keeping to a straight and orderly line (I didn't say it was a realistic example). Once across the school yard, we can once more check everyone's position in line (their new score). Odds are, all or most students will end up with the same "score" they had before. It doesn't mean no progress has been made, on the contrary, everyone is now on the other side of the school yard, but their relative positions haven't changed (much). That is the situation with age-graded scores of all types. Progress is obscured because each student is ranked relative to their peers. 

This brings us to those kinds of scales that are designed specifically to reveal growth in achievement as students advance through grades: *developmental scales*. One very common type of developmental scale is the **grade-equivalent score** (GE). GE scores are typically written with a numerical grade and month, which corresponds to the average score achieved by students from the normative sample at that particular grade and month of their education. A GE score of 6.5 is the typical performance of of a 6th grader in the fifth month of the school year, on the test in question. So, unlike percentile ranks or standard scores, we would expect a student's GE score to increase year-over-year, even if their relative ability (relative to same aged peers) does not change.

GE scores also have the advantage of being quite easy to explain and easy to understand. Unfortunately, like percentile ranks and similar scales (e.g., stanines) GE scores lack the important the property of having *equal-intervals* across their range, although for a different reason. For example, most of learning to read happens in the early grades. A student varies in the rate they gain reading skill over the course of their education; gains are faster in early grades than in later grades. So, the difference in reading skill between typical 1st and 2nd graders is greater than the difference between typical 7th and 8th graders. 

A few standardized tests will report a type of developmental scale that is intended to overcome this short-coming, the *developmental standard scale*. Be forewarned that there is no conventional name for this type of score. Each test publisher may give their own developmental standard scale a name that is unique to their product. It often takes a careful review of a test's technical manual to determine whether that test offers a developmental standard score (many don't). 

Regardless, a developmental standard scale will generally reflect reality better than the grade-equivalent scale. However, the procedures for equalizing intervals are not perfect, and even a developmental standard scale probably should not be considered a true equal-interval scale. Regardless, year-over-year growth in reading will typically not be as large in the upper grades as in the lower grades when using a DS score. But, DS scores are harder to interpret than grade equivalents, and their actual scale may vary considerably from test to test. To interpret a developmental standard score, typical scores in each grade should be used as anchor points.




Of the achievement tests in use at TSS, the GRADE and GMADE provide a developmental standard score under the name of *growth scale values*. At present, these are not included in the TSS data set. In general, it is desirable for a data set to include both standard scores and developmental scores, preferably developmental standard scores. The standard score is most important for tests that a student takes only once (e.g., TSS's cognitive tests). The developmental score, preferably developmental standard score, is most important for a test the student takes repeatedly (e.g., TSS's achievement tests). 

Koretz, D. M. (2008). *Measuring Up: What educational testing really tells us*. Harvard University Press.
 
